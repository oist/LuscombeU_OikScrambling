---
title: "Distance Measures"
author: 
 - "Charles Plessy"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Distance Measures}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

# Purpose

We want to put numbers on visual impressions such as:

 - There is a lot of scrambling.
 - Aligned regions stay on their chromosome arm.
 - Aligned regions are more "off the diagonal" on short arms than on long arms.
 - â€¦

# Load R pacakges and data

```{r load_packages_and_data}
library('OikScrambling') |> suppressPackageStartupMessages()
load("BreakPoints.Rdata")
```

See `vignette("LoadGenomicBreaks", package = "OikScrambling")` for how the
different GBreaks objects are prepared.

# Declare service functions

```{r pretty_print_functions}
pairs2table6x3 <- function (x) {
  x <- as.list(x)
  cbind(
    Oki = c(Oki = NA,        Osa = x$Osa_Oki, Bar = x$Bar_Oki),
    Kum = c(Oki = x$Oki_Kum, Osa = x$Osa_Kum, Bar = x$Bar_Kum),
    Osa = c(Oki = x$Oki_Osa, Osa = NA,        Bar = x$Bar_Osa),
    Aom = c(Oki = x$Oki_Aom, Osa = x$Osa_Aom, Bar = x$Bar_Aom),
    Bar = c(Oki = x$Oki_Bar, Osa = x$Osa_Bar, Bar = NA       ),
    Nor = c(Oki = x$Oki_Nor, Osa = x$Osa_Nor, Bar = x$Bar_Nor)
  )
}

pairs2Kable6x3 <- function (df) df |> pairs2table6x3() |> round(2) |> knitr::kable()
```

# Synteny index

Ad-hoc index measuring to what extent a scaffold of the _target_ genome is
mostly aligned to a single scaffold in the _query_ genome.   See
`?GenomicBreaks::synteny_index` for details.  One limitation to the use of this
index is that it requires that at least the _query_ genome is a complete
chromosome assembly.

```{r synteny_index_def}
synteny_index
```

```{r synteny_index_run}
BiocParallel::bplapply(gbs,                 synteny_index  ) |> pairs2Kable6x3()
BiocParallel::bplapply(gbs, \(g) swap(g) |> synteny_index()) |> pairs2Kable6x3()
```

# Correlation

Ad-hoc index measuring the correlation of the coordinates of the syntenic
alignments in scaffolds of a _target_ genome and their best match in the
_query_ genome.  See `?GenomicBreaks::correlation_index` for details.  It is
intended to be more robust to the presence of uncollapsed haplotypes in the
_query_ genome but probably needs further testing.

```{r correlation_index_def}
correlation_index
```

```{r correlation_index_run}
BiocParallel::bplapply(gbs,                 correlation_index  ) |> pairs2Kable6x3()
BiocParallel::bplapply(gbs, \(g) swap(g) |> correlation_index()) |> pairs2Kable6x3()
```

# Gene Order Conservation

See `?GenomicBreaks::GOC` for details.

```{r goc_index_def}
GOC
```

```{r goc_index_run}
BiocParallel::bplapply(gbs,                 GOC  ) |> pairs2Kable6x3()
BiocParallel::bplapply(gbs, \(g) swap(g) |> GOC()) |> pairs2Kable6x3()
```

# Strand randomisation index

See `?GenomicBreaks::strand_randomisation_index` for details.

```{r strand_randomisation_index_def}
strand_randomisation_index
```

```{r strand_randomisation_index_run}
BiocParallel::bplapply(gbs,                 strand_randomisation_index  ) |> pairs2Kable6x3()
BiocParallel::bplapply(gbs, \(g) swap(g) |> strand_randomisation_index()) |> pairs2Kable6x3()
```

In order to assess the extent to which the strand randomisation index may vary in a given pair of genomes, I do a simple permutation test. Given a `GBreaks` object, reverse the strand of a randomly-selected number of alignments drawn from a random distribution.

You can evaluate the relative frequency of "flips", i.e., a possible inversion, by counting the frequency of changes in the strand. I check this quickly below. Then I use the shape of this distribution to select a random number of flips to make, then make 10,000 permutations of the alignment with the randomly-selected number of flips, then calculate strand randomization indices on these randomly-flipped alignments. 

```{r strand_randomisation_permutation_test}
# Some libraries for fitting distributions...
library(fitdistrplus) |> suppressPackageStartupMessages()
library(stats4)       |> suppressPackageStartupMessages()
library(MASS)         |> suppressPackageStartupMessages()
library(survival)     |> suppressPackageStartupMessages()
library(actuar)       |> suppressPackageStartupMessages()
library(distrMod)     |> suppressPackageStartupMessages()
library(ggplotify)    |> suppressPackageStartupMessages()
library(patchwork)    |> suppressPackageStartupMessages()


# Each pair produces a slightly different graph. However, hopefully the distribution is similar.
plotdist(gbs$Oki_Osa |> strand() |> runLength() |> as.numeric(), demp=TRUE)
plotdist(gbs$Oki_Bar |> strand() |> runLength() |> as.numeric(), demp=TRUE)
plotdist(gbs$Osa_Bar |> strand() |> runLength() |> as.numeric(), demp=TRUE)
plotdist(gbs$Oki_Kum |> strand() |> runLength() |> as.numeric(), demp=TRUE)

descdist(gbs$Oki_Osa |> strand() |> runLength() |> as.numeric(), boot = 10000)
descdist(gbs$Oki_Bar |> strand() |> runLength() |> as.numeric(), boot = 10000)
descdist(gbs$Osa_Bar |> strand() |> runLength() |> as.numeric(), boot = 10000)
descdist(gbs$Oki_Kum |> strand() |> runLength() |> as.numeric(), boot = 10000)


# Fit a numeric vector (i.e., the strand run length) to a number of probability distributions
fit_distributions <- function(vec, distributions = c("nbinom", "pois", "lnorm", "norm", "exp", "gamma", "llogis", "weibull"), method="mle", ...) {
  setNames(lapply(distributions, function(d) {
    fitdist(vec, d, method=method, ...)
  }), distributions)
}

# We can evaluate them one at a time:
fit_then_compplot <- function(gr, method="mle", distributions=c("nbinom", "pois", "lnorm", "norm", "exp", "gamma", "llogis", "weibull"), fitlwd=0.5, fitpch=16, ...) {
  srl <- gr |> strand() |> runLength() |> as.numeric()
  f <- fit_distributions(srl, distributions=distributions, method=method, ...)
  p1 <- denscomp(f, plotstyle='ggplot', addlegend = F, fitlwd = fitlwd)
  p2 <- qqcomp(  f, plotstyle='ggplot', addlegend = T, fitpch=fitpch)
  p3 <- cdfcomp( f, plotstyle='ggplot', addlegend = F, fitlwd = fitlwd) + theme_grey() + theme(legend.position='none')
  p4 <- ppcomp(  f, plotstyle='ggplot', addlegend = T, fitpch=fitpch)
  (p1 | p2 ) / ( p3 | p4)
}

# You can evaluate the different models graphically. These plots are a little busy, though.
fit_then_compplot(gbs$Oki_Osa)
fit_then_compplot(gbs$Oki_Bar)
fit_then_compplot(gbs$Osa_Bar)
fit_then_compplot(gbs$Oki_Kum)


# Obtain goodness-of-fit statistics for each model to see which fits data best.
# Then we can use the best-fitting model as a probability distribution to randomly
# induce a realistic number of "flips", generating a randomly-flipped GBreaks object.
(gbs$Oki_Osa |> strand() |> runLength() |> as.numeric() |> fit_distributions(method="mle") |> gofstat())
(gbs$Oki_Bar |> strand() |> runLength() |> as.numeric() |> fit_distributions(method="mle") |> gofstat())
(gbs$Osa_Bar |> strand() |> runLength() |> as.numeric() |> fit_distributions(method="mle") |> gofstat())
(gbs$Oki_Kum |> strand() |> runLength() |> as.numeric() |> fit_distributions(method="mle") |> gofstat())


# Similar to the above, but only plots the comparisons.
just_compplot <- function(cpl, method="mle", distributions=c("nbinom", "pois", "lnorm", "norm", "exp", "gamma", "llogis", "weibull"), fitlwd=0.5, fitpch=16, ...) {
  p1 <- denscomp(cpl, plotstyle='ggplot', addlegend = F, fitlwd = fitlwd)
  p2 <- qqcomp(  cpl, plotstyle='ggplot', addlegend = T, fitpch=fitpch)
  p3 <- cdfcomp( cpl, plotstyle='ggplot', addlegend = F, fitlwd = fitlwd) + theme_grey() + theme(legend.position='none')
  p4 <- ppcomp(  cpl, plotstyle='ggplot', addlegend = T, fitpch=fitpch)
  (p1 | p2 ) / ( p3 | p4)
}

# Now that there is some indication that log-normal is the best distribution for making
# models, we can try different fitting methods, selecting the best from among them.
# The available fitting methdos include:
#  mle - maximum likelihood estimation
#  mge - maximum goodness-of-fit
#  mme - moment matching
#  mse - maximum spacing estimation
# 
# MGE has a number of methods available. Sometimes the plots are too complicated, so you can remove them.
#   c('mge_cvm', 'mge_ks', 'mge_ad', 'mge_adr', 'mge_adl', 'mge_ad2r', 'mge_ad2l', 'mge_ad2')
fit_methods <- function(vec, distribution="lnorm", includeMethods=c('mle', 'mge', 'mme', 'mse'), excludeMethods=NULL ) {
  m <- list()
  if('mle' %in% includeMethods){
    m <- append(m, list('mle'=fitdist(vec, distribution, method='mle')))
  }
  if('mge' %in% includeMethods){
    m <- append(m, list(
          'mge_cvm'=fitdist(vec, distribution, method='mge', gof='CvM'),
          'mge_ks'=fitdist(vec, distribution, method='mge', gof='KS'),
          'mge_ad'=fitdist(vec, distribution, method='mge', gof='AD'),
          'mge_adr'=fitdist(vec, distribution, method='mge', gof='ADR'),
          'mge_adl'=fitdist(vec, distribution, method='mge', gof='ADL'),
          'mge_ad2r'=fitdist(vec, distribution, method='mge', gof='AD2R'),
          'mge_ad2l'=fitdist(vec, distribution, method='mge', gof='AD2L'),
          'mge_ad2'=fitdist(vec, distribution, method='mge', gof='AD2')
          )
        )
  }
  if('mme' %in% includeMethods){
    m <- append(m, list('mme'=fitdist(vec, distribution, method='mme')))
  }
  if('mse' %in% includeMethods){
    m <- append(m, list('mse'=fitdist(vec, distribution, method='mse')))
  }
  m <- m[! names(m) %in% excludeMethods]
  m
}
c_oki_osa <- gbs$Oki_Osa |> strand() |> runLength() |> as.numeric() |> fit_methods()
c_oki_osa |> just_compplot()
c_oki_osa |> gofstat()

# As above, but for optimization testing.
fit_optims <- function(vec, distribution="lnorm", method='mle', maxit=10000, ...) {
  list('nelder_mead'=fitdist(vec, distribution, optim.method='Nelder-Mead', method=method, ...),
       'bfgs'=fitdist(vec, distribution, optim.method='BFGS', method=method, ...),
       'sann'=fitdist(vec, distribution, optim.method='SANN', method=method, ...),
       'cg'=fitdist(vec, distribution,   optim.method='CG', control=list(maxit=maxit), method=method, ...)
   )
}
o_oki_osa <- gbs$Oki_Osa |> strand() |> runLength() |> as.numeric() |> fit_optims()
o_oki_osa |> just_compplot()
o_oki_osa |> gofstat()


# Based on the 
# gbs$Oki_Osa |> strand() |> runLength() |> as.numeric() |> fit_methods(excludeMethods = c('mse', 'mme', 'mge_cvm', 'mge_ks', 'mge_ad', 'mge_ad2r', 'mge_ad2l', 'mge_ad') ) |> just_compplot()


# All 3 agree that a log-normal distribution best describes the size of strands.
# Use this to generate randomly-generated GRanges.
randomly_flip_strand <- function(gr, times=1000, seed=1992, distribution="lnorm", method="mle", ...) {
  # Fit the strand run length to a log-normal distribution.
  fit <- gr |> strand() |> runLength() |> as.numeric() |> fit_distributions(distributions = distribution, method=method, ...)
  fit <- fit[[1]]
  fit_mean <- tmp$estimate[1]
  fit_sd   <- tmp$estimate[2]
  
  rng <- set.seed(seed)
  
  # Make a phony GRanges (with the same dimensions as input) where all strands are positive.
  # Then iteratively flip them according to the probability of a flip.
  phony_gr <- gr
  strand(phony_gr) <- "+"
  
  # Flip a GRanges according to a probability distribution.
  # We fit the strand run length to a distribution above. Then, use the estimated parameters of the
  # fitted distribution to generate values (i.e., segments of length N), continuing until the length
  # of all segments equals the length of the original GRanges. Then re-write the strand information
  # according to the segments.
  generate_random_values <- function(distribution) {
    total_length <- length(gr)
    while(total_length < total_length){
      
    }
  }
  
  randomly_flipped_grs <- lapply(1:times, function(it){
    flip_granges(phony_gr, distribution=distribution)
  })
}
randomly_flip_strand(gbs$Oki_Osa)

detach('package:fitdistrplus')
detach('package:survival')
detach('package:actuar')
detach('package:distrMod')
detach('package:ggplotify')
detach('package:patchwork')
```


# Strand proportion index

Attempts to use pure proportions lead to scores drifting towards 0.5 when
comparing fragmented assemblies, as the fragments are not oriented.

```{r strand_proportion_index_def}
strand_proportion_index <- function(gb) {
  gbl <- split(gb, droplevels(seqnames(gb)))
  # Calculate an index for each sequence feature
  idx <- sapply(gbl, \(x) {
    onPlus  <- sum(width(x[strand(x) == '+']))
    onMinus <- sum(width(x[strand(x) == '-']))
    onPlus / (onPlus + onMinus)
  })
  # Average by the sum of all widths
  weighted.mean(idx, sum(width(gbl)))
}
```

```{r strand_proportion_index_run}
BiocParallel::bplapply(gbs,                 strand_proportion_index  ) |> pairs2Kable6x3()
BiocParallel::bplapply(gbs, \(g) swap(g) |> strand_proportion_index()) |> pairs2Kable6x3()
```

# Strand proportion majority index

One possible solution to the problem above could be to report the highest
proportion, so that the value returned is not sensitive to orientation.  But
it scales the index between 0.5 and 1, which may be counter-intuitive.

```{r strand_proportion_majority_index_def}
strand_proportion_majority_index <- function(gb) {
  gbl <- split(gb, droplevels(seqnames(gb)))
  # Calculate an index for each sequence feature
  idx <- sapply(gbl, \(x) {
    onPlus  <- sum(width(x[strand(x) == '+']))
    onMinus <- sum(width(x[strand(x) == '-']))
    p <- onPlus / (onPlus + onMinus)
    if(p < 0.5) p <- 1 - p
    p
  })
  # Average by the sum of all widths
  weighted.mean(idx, sum(width(gbl)))
}
```

```{r strand_proportion_majority_index_run}
BiocParallel::bplapply(gbs,                 strand_proportion_majority_index  ) |> pairs2Kable6x3()
BiocParallel::bplapply(gbs, \(g) swap(g) |> strand_proportion_majority_index()) |> pairs2Kable6x3()
```
